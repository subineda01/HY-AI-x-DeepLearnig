# ë”¥ëŸ¬ë‹ ê¸°ë°˜ í…ìŠ¤íŠ¸ì˜ ê°ì • ë¶„ì„
Members : 

ì´ê°€ë¹ˆ, í™”í•™ê³¼, gabin0713@hanyang.ac.kr

ì¥ìˆ˜ë¹ˆ, ìˆ˜í•™ê³¼, subineda01@hanyang.ac.kr

ë°•ìŠ¹í˜„, ê²½ì˜í•™ë¶€, boyojeck@hanyang.ac.kr

ì´ìƒë°±, ê¸°ê³„ê³µí•™ë¶€, leesangbaek98@naver.com



ğŸ” ëª©ì°¨
1. Proposal
2. DataSets
3. Methodology & Evaluation
4. Conclusion-discussion
5. Related Works

-------------------------
# I.Proposal
- Why are you doing this?

 &ensp;ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ê°ì • ì¸ì‹ ê¸°ìˆ ì€ ìš°ë¦¬ê°€ ë¯¸ì²˜ ì•Œì§€ ëª»í–ˆë˜ ì¸ê°„ì˜ ê°ì • íŒ¨í„´ê³¼ ì‹¬ë¦¬ ìƒíƒœë¥¼ ë” ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°ì • ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í†µí•´ ê°œì¸ì ì¸ ì¸¡ë©´ìœ¼ë¡œëŠ” ì‚¬ëŒë“¤ì˜ ì˜¨ë¼ì¸ í™œë™ê³¼ ì†Œì…œ ë¯¸ë””ì–´ ê²Œì‹œë¬¼ì—ì„œ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ìš°ìš¸ì¦, ë¶ˆì•ˆ, ìŠ¤íŠ¸ë ˆìŠ¤ ë“±ìœ¼ ì •ì‹  ê±´ê°• ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬ ê°€ëŠ¥í•˜ê²Œ í•´ì¤€ë‹¤. ë˜í•œ, ì¸ê³µì§€ëŠ¥ ë¹„ì„œë‚˜ êµìœ¡ìš© ë¡œë´‡ ë“±ì„ í†µí•´ í•™ì—…ì ìœ¼ë¡œë‚˜ ì‚¬ë¬´ì ì¸ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”ì‹œí‚¤ê³ , ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš©(HCI)ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‚¬íšŒì ì¸ ì¸¡ë©´ìœ¼ë¡œëŠ” ê¸°ì—…ë“¤ì´ ê³ ê° ì„œë¹„ìŠ¤ì—ì„œ ê°ì • ì¸ì‹ ê¸°ìˆ ì„ í†µí•´ ê³ ê°ì˜ ê°ì • ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì•…í•˜ì—¬, í”¼ë“œë°± ìˆ˜ìš©ì´ ìš©ì´í•˜ë‹¤. ì†Œì…œ ë¯¸ë””ì–´ì—ì„œ ë°œìƒí•˜ëŠ” í˜ì˜¤ ë°œì–¸ì´ë‚˜ ì‚¬ì´ë²„ ë¶ˆë§ì„ ê°ì§€í•˜ì—¬, ì´ë¥¼ ì‚¬ì „ì— ë°©ì§€í•¨ìœ¼ë¡œì¨ ì‚¬íšŒì  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ, ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ê°ì • ì¸ì‹ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì‚¬íšŒì , í•™ë¬¸ì , ê²½ì œì  ì´ì ì„ ì œê³µí•˜ë©°, ì¸ë¥˜ì˜ ì‚¶ì„ ë” ë‚˜ì€ ë°©í–¥ìœ¼ë¡œ ì´ëŒì–´ê°ˆ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ë¥¼ ì§€ì†í•˜ê³  ë°œì „ì‹œí‚¤ëŠ” ê²ƒì€ ìš°ë¦¬ì˜ ì‚¶ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ê³ , ë” ë‚˜ì€ ì‚¬íšŒë¥¼ ë§Œë“œëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒì´ë¼ íŒë‹¨í•˜ì—¬ ì„ ì •í•˜ê²Œ ë˜ì—ˆë‹¤.

- What do you want to see at the end?

 &ensp;ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì • ì¸ì‹ ê¸°ìˆ ì„ í†µí•´ ì •ì‹  ê±´ê°• ê´€ë¦¬, ê³ ê° ì„œë¹„ìŠ¤, ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš©, ì‚¬íšŒì  ë¬¸ì œ í•´ê²° ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‹¤ì§ˆì ì¸ ë³€í™”ë¥¼ ì´ë£¨ê³ ì í•œë‹¤. ì´ ê¸°ìˆ ì€ ì‚¬ëŒë“¤ì˜ ì˜¨ë¼ì¸ í™œë™ê³¼ ì†Œì…œ ë¯¸ë””ì–´ ê²Œì‹œë¬¼ì—ì„œ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ìš°ìš¸ì¦, ë¶ˆì•ˆ, ìŠ¤íŠ¸ë ˆìŠ¤ ë“±ì˜ ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•˜ê³  ì˜ˆë°©í•˜ë©°, ê³ ê°ì˜ ê°ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ ë” ë‚˜ì€ ì„œë¹„ìŠ¤ì™€ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤. ë˜í•œ, í˜ì˜¤ ë°œì–¸ì´ë‚˜ ì‚¬ì´ë²„ ë¶ˆë§ì„ ê°ì§€í•˜ì—¬ ì•ˆì „í•œ ì¸í„°ë„· í™˜ê²½ì„ ì¡°ì„±í•˜ê³ , ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬íšŒì  íŠ¸ë Œë“œì™€ ê°ì • ë³€í™”ë¥¼ íŒŒì•…í•¨ìœ¼ë¡œì¨ íš¨ê³¼ì ì¸ ì •ì±… ìˆ˜ë¦½ì„ ì§€ì›í•œë‹¤. ê¶ê·¹ì ìœ¼ë¡œ, ì´ ì—°êµ¬ë¥¼ í†µí•´ ì¸ê°„ ê°ì •ì˜ ë³µì¡ì„±ì„ ì´í•´í•˜ê³  ë‹¤ì–‘í•œ í•™ë¬¸ ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ì´ë¡ ê³¼ ì‹¤ì²œ ë°©ì•ˆì„ ê°œë°œí•˜ë©°, ì˜ˆìˆ ê³¼ ë¬¸í™” ì—°êµ¬ ë“±ì—ì„œë„ í˜ì‹ ì ì¸ ë³€í™”ë¥¼ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆê¸°ë¥¼ ê¸°ëŒ€í•œë‹¤. ì´ë¥¼ ìœ„í•´, ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¯¸ë˜ì˜ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ì—¬, ì‚¬ëŒë“¤ì´ ë” ë‚˜ì€ ì˜ì‚¬ê²°ì •ì„ í•  ìˆ˜ ìˆë„ë¡ ë•ê³ ì í•˜ëŠ” ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ê³ ì í•œë‹¤.

-------------------------
# II.DataSets
ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜ì˜ ë…ë¦½ ì—°êµ¬ì†Œì¸ DAIR.AIì—ì„œ ì œê³µí•˜ëŠ” 'Emotion Dataset'ì„ ì´ìš©í•œë‹¤.

í•´ë‹¹ ë°ì´í„°ëŠ” twitter APIë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ì˜ì–´ë¬¸ì¥ì„ ì—¬ì„¯ê°€ì§€ ê¸°ë³¸ê°ì •ë“¤(anger, fear, joy, love, sadness, surprise)ë¡œ ë¶„ë¥˜ë˜ì—ˆë‹¤.

ì„ í–‰ ì—°êµ¬ì¸ â€˜CARER: Contextualized Affect Representations for Emotion Recognitionâ€™ì˜ ì ‘ê·¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ê°€ ê°€ê³µì²˜ë¦¬ ëœë‹¤.

Prior research Link : <https://aclanthology.org/D18-1404.pdf>  

Data link : <https://github.com/dair-ai/emotion_dataset>

## DataSets info

'Emotion Dataset' ì¤‘ í•™ìŠµì„ ìœ„í•´ ì œê³µí•œ split dataë¥¼ ì‚¬ìš©í•œë‹¤.

![ë°ì´í„°íŒŒì¼](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/dataset.png)

train.csv(16,000), validation.csv(2,000), test.csv(2,000)
-> ì´ 20,000ê°œì˜ ë°ì´í„° (1968KB)

<train.csvì˜ ë°ì´í„° ê·¸ë˜í”„>

<img src="https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/distribution%20of%20label.png?raw=true" width="500" height="500"/>

## Data example
```sh
"text" : "im feeling quite sad and sorry for myself but ill snap out of it soon",
"label": 0
```

## Features
- text : í•œ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ string í˜•íƒœì˜ feature
- label : ê°ì •ì„ ë¶„ë¥˜í•œ ë¼ë²¨ë¡œ int í˜•íƒœì˜ feature, 6ê°€ì§€ ìƒíƒœë¥¼ í‘œí˜„

| Emotion | label |
| ------- | ------- |
| sadness | 0 |
| joy | 1 |
| love | 2 |
| anger | 3 |
| fear | 4 |
| surprise | 5 |



-----------------------
# III.Methodology & Evaluation
## 1. Lstm classification
    
ë¬¸ì¥ê³¼ ê°™ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì£¼ë¡œ RNN(ìˆœí™˜ì‹ ê²½ë§, Recurrent Neural Network)ì„ ì‚¬ìš©í•œë‹¤.
  
ë¬¸ì¥ ì† ì´ì „ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ê¸°ì–µí•˜ëŠ” ê²ƒì„ ì‹œì‘ìœ¼ë¡œ, ë‹¤ìŒì˜ ìƒˆë¡œìš´ ë‹¨ì–´ì™€ì˜ ì •ë³´ë¥¼ í•©ì³ì„œ ì²˜ë¦¬í•˜ë©´ì„œ AIëŠ” ë‹¨ì–´ì˜ ìˆœì„œì™€ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

ì´ì „ì˜ ë…¸ë“œì—ì„œ ë‚˜ì˜¨ í•œê°œì˜ ì •ë³´ì™€ ìƒˆë¡œìš´ ë‹¨ì–´ì˜ ì •ë³´ë§Œì„ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì—, ê¸´ ë¬¸ì¥ì— ëŒ€í•˜ì—¬ ì²˜ë¦¬í•  ë•Œ ì•ì˜ ì •ë³´ë¥¼ ì˜ ê¸°ì–µí•˜ì§€ ëª»í•  ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

![image](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/vaniila_rnn_and_different_lstm_ver2.png?raw=true)
    
ì „í†µì ì¸ RNNì˜ ë‹¨ì ì„ ë³´ì™„í•œ RNNì˜ ì¼ì¢…ì„ LSTM(ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬, Long Short-Term Memory)ë¼ê³  í•œë‹¤. 
  
í•´ë‹¹ ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ì…€ì— ì…ë ¥ ê²Œì´íŠ¸, ë§ê° ê²Œì´íŠ¸, ì¶œë ¥ ê²Œì´íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶ˆí•„ìš”í•œ ê¸°ì–µì„ ì§€ìš°ê³ , ê¸°ì–µí•  ê²ƒì„ ìœ ì§€ì‹œí‚¤ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

ë§ê° ê²Œì´íŠ¸ì— ì˜í•´ ì¼ë¶€ ê¸°ì–µì„ ìƒê³ , ì…ë ¥ê²Œì´íŠ¸ì— ì˜í•´ ìœ ì§€ì‹œí‚¬ ê¸°ì–µì„ ì €ì¥í•œ ì…€ ìƒíƒœ $`C_t`$ê°€ ì¶”ê°€ë˜ì–´ ë‹¤ìŒ ë©”ëª¨ë¦¬ ì…€ë¡œ ì „íŒŒëœë‹¤. 

í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” LSTMë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ê³ , Clssification ëª¨ë¸ì„ ë¶™ì´ëŠ” ê²ƒìœ¼ë¡œ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤.

ëª¨ë¸ì´ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ”, ë¬¸ì¥ì´ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í°ì˜ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬ ë˜ì–´ì•¼ í•œë‹¤.(Tokenization)

í”„ë¡œì íŠ¸ì—ì„œëŠ” ```torchtext.data.utils``` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. 

```
Text: Hello, world!
Tokens: ['hello', ',', 'world', '!']
Token indices: [3, 4, 5, 6]
```
í† í°í™”ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê³ ìœ í•œ í† í°ì„ ìˆ˜ì§‘í•˜ê³ , ì´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘í•˜ì—¬ ì‚¬ì „ì„ í˜•ì„±í•œë‹¤.

ì´í›„ ì„ë² ë”©(Word Embedding)ì„ í†µí•´ ê° í† í°ì„ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ PyTorchì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•œë‹¤.

ì„ë² ë”© ëœ í† í°ì„ ì´ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì¬êµ¬ì„±í•˜ê³ (Token indices) ë¬¸ì¥ í•™ìŠµì„ ì§„í–‰ì‹œí‚¨ë‹¤. 

Activation Functionì€ ê°€ì¥ ë³´í¸ì ì¸ ```Adam```ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, loss functionë¡œëŠ” ```nn.CrossEntropyLoss()```ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.

```nn.CrossEntropyLoss()```ì— í¬í•¨ëœ ```softmax``` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ 6ê°€ì§€ì˜ ê°ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ì˜€ë‹¤.



í•™ìŠµë¨¸ì‹  : 12th Gen Intel(R) Core(TM) i7-12700H, ddr5 16GB

### total code
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.utils import get_tokenizer
import torch.nn as nn
import torch.optim as optim

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
train_data = pd.read_csv('train.csv')
val_data = pd.read_csv('validation.csv')

# í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬
train_sentences = train_data['text'].values
train_labels = train_data['label'].values

# ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬
val_sentences = val_data['text'].values
val_labels = val_data['label'].values

label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_labels)
val_labels = label_encoder.transform(val_labels)

tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
    for text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(train_sentences), specials=["<pad>", "<unk>"])
vocab.set_default_index(vocab["<unk>"])

class TextDataset(Dataset):
    def __init__(self, texts, labels, vocab, tokenizer):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        tokens = self.tokenizer(text)
        token_ids = [self.vocab[token] for token in tokens]
        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)

def collate_batch(batch):
    text_list, label_list = [], []
    for _text, _label in batch:
        text_list.append(torch.tensor(_text, dtype=torch.long))
        label_list.append(torch.tensor(_label, dtype=torch.long))
    return pad_sequence(text_list, batch_first=True, padding_value=vocab["<pad>"]), torch.stack(label_list)

train_dataset = TextDataset(train_sentences, train_labels, vocab, tokenizer)
val_dataset = TextDataset(val_sentences, val_labels, vocab, tokenizer)

train_dataloader = DataLoader(train_dataset, batch_size='', shuffle=True, collate_fn=collate_batch)  # ë°°ì¹˜ì‚¬ì´ì¦ˆ ì¡°ì •
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, lstm_units, num_classes, dropout_rate):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab["<pad>"])
        self.lstm = nn.LSTM(embed_dim, lstm_units, num_layers = '', batch_first=True) # LSTM ë ˆì´ì–´ ìˆ˜ ì¡°ì •
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(lstm_units, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x, (hidden, _) = self.lstm(x)
        x = self.dropout(hidden[-1])
        x = self.fc(x)
        return x

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
embed_dim = ''
lstm_units = ''
dropout_rate = ''
learning_rate = ''
num_epochs = ''

# ëª¨ë¸ ì´ˆê¸°í™”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMClassifier(vocab_size=len(vocab), embed_dim=embed_dim, lstm_units=lstm_units, num_classes=6, dropout_rate=dropout_rate).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# ì†ì‹¤ ê¸°ë¡ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
train_losses = []
val_losses = []

# ëª¨ë¸ í•™ìŠµ
model.train()
for epoch in range(num_epochs):
    train_loss = 0.0
    for texts, labels in train_dataloader:
        texts, labels = texts.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # ì—í¬í¬ë³„ í‰ê·  í›ˆë ¨ ì†ì‹¤ ê³„ì‚°
    train_loss /= len(train_dataloader)
    train_losses.append(train_loss)

    # ê²€ì¦ ë°ì´í„° ì†ì‹¤ ê³„ì‚°
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for texts, labels in val_dataloader:
            texts, labels = texts.to(device), labels.to(device)
            outputs = model(texts)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

    # ì—í¬í¬ë³„ í‰ê·  ê²€ì¦ ì†ì‹¤ ê³„ì‚°
    val_loss /= len(val_dataloader)
    val_losses.append(val_loss)

    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}')

# ì†ì‹¤ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='blue')
plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train and Validation Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
def evaluate_model(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for texts, labels in dataloader:
            texts, labels = texts.to(device), labels.to(device)
            outputs = model(texts)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = correct / total
    return accuracy

'''
# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), 'model_weights.pth')
torch.save(model, 'model.pth')
'''

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
test_data = pd.read_csv('test.csv')
test_sentences = test_data['text'].values
test_labels = test_data['label'].values
test_labels = label_encoder.transform(test_labels)

test_dataset = TextDataset(test_sentences, test_labels, vocab, tokenizer)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€
test_accuracy = evaluate_model(model, test_dataloader)
print(f'Test Accuracy: {test_accuracy}')

```

## Result

Default parameterëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•˜ì˜€ë‹¤. 

```
lstm_layer = 1
Batch_size = 32
embed_dim = 128
lstm_units = 128
dropout_rate = 0.3
learning_rate = 1e-3
num_epochs = 10
```
![default](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/%EA%B8%B0%EB%B3%B8(10%EC%97%90%ED%8F%AD,1%EB%A0%88%EC%9D%B4%EC%96%B4)0.594.png?raw=true)
```Accuracy : 0.594```

Lossê·¸ë˜í”„ì—ì„œ epochê°€ ì§€ë‚  ë•Œë§ˆë‹¤ Train Lossê°€ ê°ì†Œí•˜ê³ , Validation Loss ë˜í•œ ìœ ì˜ë¯¸í•˜ê²Œ ê°ì†Œí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

í•˜ì§€ë§Œ ê°ì†Œì˜ ì •ë„ê°€ í¬ì§€ ì•Šê³ , ì •í™•ë„ ë˜í•œ 59.4% ìˆ˜ì¤€ìœ¼ë¡œ ë†’ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ë‹¤ìŒê³¼ ê°™ì€ hyper parameter tuningì„ ì§„í–‰í•˜ì˜€ë‹¤.

### (1) learning_rate = 1e-2

![lr](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/10epoch_1lstm_lr001_0.454.png?raw=true)
```Accuracy : 0.454```

### (2) Batch_size = 64

![Batch](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/10epoch_1lstm_64batch_accu62.35.png?raw=true)
```Accuracy : 0.6235```

### (3) lstm_layer = 2

![layer](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/2%EB%A0%88%EC%9D%B4%EC%96%B40.872.png?raw=true)
```Accuracy : 0.872```

### (4) num_epochs = 20

![epoch](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/20epoch%EC%9C%BC%EB%A1%9C%EB%8A%98%EB%A6%BC0.8725.png?raw=true)
```Accuracy : 0.8725```

ìœ„ 4ê°€ì§€ parameter tuning ì¤‘ ê°€ì¥ íš¨ê³¼ì ì´ì—ˆë˜ (3)```lstm_layer```ì™€ (4)```num_epochs```ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤. 

![last](https://github.com/subineda01/HY-AI-x-DeepLearnig/blob/main/image/15%EC%97%90%ED%8F%AD,2%EB%A0%88%EC%9D%B4%EC%96%B40.9005.png?raw=true)
```Accuracy : 0.9005```

ì´ì™¸ì—ë„ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì—¬ëŸ¬ê°€ì§€ hyper parameter tuningì„ ì‹œë„í•˜ì˜€ìœ¼ë‚˜, ë” ì´ìƒ ì˜¬ë¼ê°€ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ëª¨ë¸ì„ íƒìƒ‰í•˜ì˜€ë‹¤.

ê·¸ ê²°ê³¼ Bertë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ëª¨ë¸ì„ êµ¬ì„±í•˜ì˜€ë‹¤. 

-----------------------

## 2. BertForSequenceClassification
 &ensp;BERT(Bidirectional Encoder Representations from Transformers)ëŠ” Goolgeì—ì„œ ê°œë°œí•œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ë¡œ, 2018ë…„ì— ë°œí‘œë˜ì—ˆë‹¤. í…ìŠ¤íŠ¸ì˜ ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•˜ëŠ” ë° ë›°ì–´ë‚˜ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ì§€ë‹ˆê³  ìˆë‹¤. 
BERTëŠ” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ì—ì„œ í•™ìŠµë˜ì–´ ìˆ˜ì–µê°œì˜ ë‹¨ì–´ë¥¼ í•™ìŠµí•˜ê³  ìˆë‹¤. ì‚¬ì „ í•™ìŠµëœ ì´í•´ë„ë¥¼ ì´ìš©í•´ ì „ì´ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ ëª©ì ì— ì í•©í•œ ë›°ì–´ë‚œ ëª¨ë¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤. 
BertForSequenceClassification ëª¨ë¸ì€ Hugging Faceì˜ Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•œ BERT ê¸°ë°˜ ëª¨ë¸ì´ë‹¤. ì´ ëª¨ë¸ì€ BERTì˜ ê¸°ë³¸ ì•„í‚¤í…ì²˜ ìœ„ì— ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤.

ì „ì²´êµ¬ì¡°

![image](https://github.com/subineda01/HY-AI-x-DeepLearnig/assets/144909753/18a71006-4452-4258-93b4-3a8a0c0ff3ab)

 &ensp;ëª¨ë¸ì€ í¬ê²Œ ë‘ê°€ì§€ êµ¬ì¡°ì¸ BertModelê³¼ Classifierë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. BertModelì€ Transformer layerê°€ ì—¬ëŸ¬ê²¹ìœ¼ë¡œ ìŒ“ì—¬ìˆëŠ” ë³¸ì²´ì´ë‹¤. ì´ëŠ” BertEmbedding ë¶€ë¶„ê³¼ BertEncoderë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆë‹¤. 

### BertEmbedding

![image](https://github.com/subineda01/HY-AI-x-DeepLearnig/assets/144909753/589d2e7d-aeda-44d5-8a0c-9f73000fd8b6)

BertEmbeddingì€ ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í† í°í™” ì‹œí‚¤ê³  token, segment, positionì„ ì„ë² ë”©í•˜ì—¬ ê°’ìœ¼ë¡œ ë§Œë“¤ê³  ë”í•´ì„œ ë°˜í™˜í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.

ì…ë ¥ ì„ë² ë”©(Input Embeddings):
   * Token Embedding : ê° í† í°ì— ëŒ€í•œ ê³ ìœ í•œ ì„ë² ë”© ë²¡í„°
   * Segment Embedding : ë¬¸ì¥ì´ ë‘ê°œì¼ ë•Œ ì²« ë¬¸ì¥ê³¼ ë‘ ë²ˆì§¸ ë¬¸ë‹¹ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì„ë² ë”© ë²¡í„°
   * Position Embedding : ê° í† í°ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ë² ë”© ë²¡í„°. ë¬¸ì¥ ë‚´ì—ì„œ ê° í† í°ì˜ ìˆœì„œë¥¼ ëª¨ë¸ì´ ì•Œ ìˆ˜ ìˆê²Œ í•œë‹¤.

### BertEncoder

BERTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸ì˜ ì¸ì½”ë” ë¶€ë¶„ë§Œ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ì—¬ëŸ¬ ì¸µì˜ ì¸ì½”ë” ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

#### - íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ê°œìš”

BERTì˜ ì¸ì½”ë”ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë¸”ë¡ì˜ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ëœë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ëŠ” ì—¬ëŸ¬ ì¸µì˜ ì¸ì½”ë” ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ë¸”ë¡ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.

1. Multi-Head Self-Attention Mechanism:
   - Query, Key, Value í–‰ë ¬ì„ ê³„ì‚°í•˜ê³ , Attention ì ìˆ˜ë¥¼ í†µí•´ í† í° ìŒì˜ ê´€ê³„ë¥¼ í•™ìŠµí•œë‹¤.
     
![QKV Calculation](https://latex.codecogs.com/svg.latex?Q%20%3D%20XW_Q%2C%20%5Cquad%20K%20%3D%20XW_K%2C%20%5Cquad%20V%20%3D%20XW_V)

2. Position-wise Feed-Forward Neural Network:
   - ë‘ ê°œì˜ ì„ í˜• ë³€í™˜ê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¡œ í•˜ì—¬ ì™„ì „ ì—°ê²° ì‹ ê²½ë§ì„ êµ¬ì„±í•œë‹¤.
     
![Feed-Forward Neural Network](https://latex.codecogs.com/svg.latex?%5Ctext%7BFFN%7D(x)%20%3D%20%5Ctext%7Bmax%7D(0%2C%20xW_1%20%2B%20b_1)W_2%20%2B%20b_2)
  
 &ensp;ì´ì™€ ê°™ì´ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ê³  ì„ë² ë”©ì„ í†µí•´ ëª¨ë¸ì— ì…ë ¥í•˜ëŠ” ê³¼ì •ì€ LSTM ëª¨ë¸ì—ì„œì˜ ì„ë² ë”© ê³¼ì •ê³¼ ìœ ì‚¬í•˜ë‹¤. BERT ëª¨ë¸ ë˜í•œ ì´ë¥¼ í†µí•´ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ë³µì¡í•œ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ , í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

ì´ ì¸ì½”ë” ë ˆì´ì–´ë“¤ì€ ì…ë ¥ ì„ë² ë”©ì„ ì ì§„ì ìœ¼ë¡œ ë” ë³µì¡í•œ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ë©°, ìµœì¢…ì ìœ¼ë¡œ ì…ë ¥ ì‹œí€¸ìŠ¤ì˜ ê° í† í°ì— ëŒ€í•œ í’ë¶€í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ í¬í•¨í•œ ê³ ì°¨ì› ë²¡í„° í‘œí˜„ì„ ì¶œë ¥í•œë‹¤.   

3. ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” (Residual Connections and Layer Normalization)

ê° íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë¸”ë¡ì€ ë‘ ê°œì˜ ì„œë¸Œë ˆì´ì–´(Sublayer)ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 
ê° ì„œë¸Œë ˆì´ì–´ í›„ì— ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµì„ ì•ˆì •í™”í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. 


+)ë¶€ê°€ ì„¤ëª…

- **ì–´í…ì…˜ ì ìˆ˜ ê³„ì‚°**: Queryì™€ Keyì˜ ë‚´ì ì„ í†µí•´ ê° í† í° ìŒì˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ìŠ¤ì¼€ì¼ë§ í›„ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì–»ëŠ”ë‹¤.
  
   ![Attention Score](https://latex.codecogs.com/svg.latex?%5Ctext%7BAttention%7D(Q%2C%20K%2C%20V)%20%3D%20%5Ctext%7Bsoftmax%7D%5Cleft(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%5Cright)V)

- **Multi-Head Attention**: ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ í—¤ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í—¤ë“œì˜ ì¶œë ¥ì„ ê²°í•©ì‹œí‚¨ë‹¤.
  
   ![Multi-Head Attention](https://latex.codecogs.com/svg.latex?%5Ctext%7BMultiHead%7D(Q%2C%20K%2C%20V)%20%3D%20%5Ctext%7BConcat%7D(%5Ctext%7Bhead%7D_1%2C%20%5Cldots%2C%20%5Ctext%7Bhead%7D_h)W_O)

ìœ„ì˜ ì›ë¦¬ë¥¼ í™œìš©í•œ```BertForSequenceClassification```ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ ëª¨ë¸ì„ ìƒì„±í•˜ì˜€ë‹¤. 

í•™ìŠµë¨¸ì‹  : Intel(R) Xeon(R) Platinum 8462Y+ ë©”ëª¨ë¦¬ 1024GB
### Total code
```python
#1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import os
import logging

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

os.environ['HF_HOME'] = "/home/subin/AI+x/cache"
os.environ['MPLCONFIGDIR'] = "/home/subin/AI+x/matplotlib_cache"

import torch
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

#2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
#EmotionDataset í´ë˜ìŠ¤ëŠ” ë°ì´í„°ì…‹ì„ ê´€ë¦¬í•˜ê³ , BERT ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜

class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

#3. ë°ì´í„° ë¡œë“œ ë° ë°ì´í„° ë¡œë” ìƒì„±
#CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , ë°ì´í„° ë¡œë”ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë“¤

def load_data(file_path):
    logging.debug(f"Loading data from {file_path}")
    df = pd.read_csv(file_path)
    texts = df['text'].tolist()
    labels = df['label'].tolist()
    logging.debug(f"Loaded {len(texts)} texts and {len(labels)} labels")
    return texts, labels

def create_data_loader(texts, labels, tokenizer, max_len, batch_size):
    logging.debug(f"Creating data loader")
    ds = EmotionDataset(
        texts=texts,
        labels=labels,
        tokenizer=tokenizer,
        max_len=max_len
    )
    return DataLoader(ds, batch_size=batch_size, num_workers=2)

#4. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
#ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ê° ì—í¬í¬(epoch)ë§ˆë‹¤ ì†ì‹¤(loss)ì„ ê¸°ë¡

def train_model(train_loader, val_loader, model, device, optimizer, scheduler, num_epochs):
    model = model.to(device)
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for d in train_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            labels = d["label"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            train_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
        train_loss = train_loss / len(train_loader)
        train_losses.append(train_loss)
        logging.debug(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}")

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for d in val_loader:
                input_ids = d["input_ids"].to(device)
                attention_mask = d["attention_mask"].to(device)
                labels = d["label"].to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss
                val_loss += loss.item()
        val_loss = val_loss / len(val_loader)
        val_losses.append(val_loss)
        logging.debug(f"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}")

    return train_losses, val_losses

#5. ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
#í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ì„ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.

def plot_losses(train_losses, val_losses, filename='losses.png'):
    plt.figure(figsize=(10, 8))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Train and Validation Loss per Epoch')
    plt.savefig(filename)
    plt.close()

#6. ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
#ëª¨ë¸ì„ í‰ê°€í•˜ê³ , ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ ë° í˜¼ë™ í–‰ë ¬ì„ ê³„ì‚°

def evaluate_model(model, data_loader, device):
    model.eval()
    predictions = []
    true_labels = []
    with torch.no_grad():
        for d in data_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            labels = d["label"].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs.logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')
    cm = confusion_matrix(true_labels, predictions)
    return accuracy, precision, recall, f1, cm

#7. í˜¼ë™ í–‰ë ¬ ê·¸ë¦¬ê¸°
#í˜¼ë™ í–‰ë ¬ì„ ì‹œê°

def plot_confusion_matrix(cm, class_names, filename='confusion_matrix.png'):
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.savefig(filename)
    plt.close()

#8. ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
#í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±

def generate_wordcloud(text, filename='wordcloud.png'):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.savefig(filename)
    plt.close()

#9. ë©”ì¸ í•¨ìˆ˜
#ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜

def main():
    try:
        # ì„¤ì •
        PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'
        MAX_LEN = 128
        BATCH_SIZE = 16
        EPOCHS = 5
        LEARNING_RATE = 2e-5

        TRAIN_DATA_FILE_PATH = "/home/subin/AI+x/data/training.csv"
        VAL_DATA_FILE_PATH = "/home/subin/AI+x/data/validation.csv"
        TEST_DATA_FILE_PATH = "/home/subin/AI+x/data/test.csv"
        MODEL_SAVE_PATH = "/home/subin/AI+x/model/bert_emotion_model.bin"
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        class_names = ["sadness","joy","love","anger","fear","surprise"]

        logging.debug("Loading data")
        train_texts, train_labels = load_data(TRAIN_DATA_FILE_PATH)
        val_texts, val_labels = load_data(VAL_DATA_FILE_PATH)
        test_texts, test_labels = load_data(TEST_DATA_FILE_PATH)

        logging.debug("Loading tokenizer")
        tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
        train_loader = create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)
        val_loader = create_data_loader(val_texts, val_labels, tokenizer, MAX_LEN, BATCH_SIZE)
        test_loader = create_data_loader(test_texts, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)

        logging.debug("Loading model")
        model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=6)
        model = model.to(device)

        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
        total_steps = len(train_loader) * EPOCHS
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

        logging.debug("Starting training")
        train_losses, val_losses = train_model(train_loader, val_loader, model, device, optimizer, scheduler, EPOCHS)

        plot_losses(train_losses, val_losses)

        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        logging.debug(f"Model saved to {MODEL_SAVE_PATH}")

    except Exception as e:
        logging.error(f"An error occurred: {e}")
        return

    try:
        logging.debug("Loading saved model for evaluation")
        model.load_state_dict(torch.load(MODEL_SAVE_PATH))
        accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")

        plot_confusion_matrix(cm, class_names)

        combined_text = ' '.join(test_texts)
        generate_wordcloud(combined_text)
        
    except Exception as e:
        logging.error(f"An error occurred during evaluation: {e}")

if __name__ == "__main__":
    main()
```
- ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬: CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , BERTì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í† í¬ë‚˜ì´ì¦ˆí•œë‹¤.
- ëª¨ë¸ í•™ìŠµ: í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ BERT ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ê° ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ì„ ê¸°ë¡í•œë‹¤.
- ëª¨ë¸ í‰ê°€: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•˜ê³ , ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ ë° í˜¼ë™ í–‰ë ¬ì„ ê³„ì‚°í•œë‹¤.
- ì‹œê°í™”: ì†ì‹¤ ê·¸ë˜í”„, í˜¼ë™ í–‰ë ¬, ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ì‹œê°í™”í•œë‹¤.

### Word Cloud
![wordcloud](https://github.com/subineda01/HY-AI-x-DeepLearnig/assets/144909753/7c09d6b2-6d35-499e-829f-e3a0c45c03dc)

word Cloud ì´ë¯¸ì§€ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë‹¨ì–´ë“¤ì˜ ë¹ˆë„ë‚˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•œë‹¤. 'feel', 'feeling', really' ë“±ì˜ ë‹¨ì–´ë“¤ì´ í¬ê²Œ í‘œì‹œë˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ë¹ˆë„ê°€ ë†’ê±°ë‚˜ ì¤‘ìš”ë„ê°€ í° ë‹¨ì–´ì„ì„ í‘œì‹œí•œë‹¤. 

### Loss Graph

![losses](https://github.com/subineda01/HY-AI-x-DeepLearnig/assets/144909753/877302d4-9837-4efa-a285-7cf732a61549)


### Confusion Matrix
![confusion_matrix](https://github.com/subineda01/HY-AI-x-DeepLearnig/assets/144909753/c118b4a3-fb0e-40ca-be83-a38c75df86da)

### Result

![image](https://github.com/subineda01/HY-AI-x-DeepLearnig/assets/144909753/cd90b260-6261-4686-971f-1b6c57635c0b)

 &ensp;ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ì‹¤í—˜í•˜ì˜€ë‹¤. í•™ìŠµë¥ ì„ 2e-3 2e-4 2e-r-5ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ í•´ë³¸ ê²°ê³¼ 2e-5ì¼ ë•Œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ì˜€ë‹¤. 
 
 &ensp;ì—í¬í¬ ìˆ˜ëŠ”  5 10 30ì„ ê°€ì§€ê³  ì‹¤í—˜ í•´ë³¸ ê²°ê³¼ ì—í¬í¬ ìˆ˜ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ validation lossê°€ ì»¤ì§ì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. 
 
 &ensp;validation setì—ì„œëŠ” ì—í¬í¬ 1 ì´í›„ë¡œ ë”ì´ìƒ í•™ìŠµì„ ì˜ í•˜ì§€ ëª»í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.(í•™ìŠµì„ ì‹œí‚¤ì§€ ì•Šì€ ìƒíƒœì—ì„œ ëª¨ë¸ì— validation.csvë¥¼ í†µê³¼ì‹œí‚¨ ê²°ê³¼ 0.135ì˜ ì •í™•ë„ê°€ ë‚˜ì™”ë‹¤. LLMì´ê¸° ë–„ë¬¸ì— 1 epochë§Œìœ¼ë¡œ ì¶©ë¶„í•œ í•™ìŠµì´ ë˜ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë˜ì—ˆë‹¤.) 
 
 &ensp;ë”°ë¼ì„œ ì—í¬í¬ì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì€ ê³¼ì í•©ì„ ë§Œë“ ë‹¤ê³  íŒë‹¨í•˜ì—¬ ì—í¬í¬ ìˆ˜ë¥¼ ì‘ê²Œ ì„¤ì •í•˜ì˜€ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë°°ì¹˜ ìˆ˜ë¥¼ 16 32 64ë¡œ ë³€ê²½í•´ ë³´ì•˜ì§€ë§Œ í° ì°¨ì´ëŠ” ì—†ì—ˆê³ , ê²°ê³¼ì ìœ¼ë¡œ ì •í™•ë„ì™€ ì¬í˜„ìœ¨ì´ ëª¨ë‘ 93%ëŒ€ë¥¼ ê¸°ë¡í•˜ì˜€ë‹¤.

ìµœì¢… í•˜ì´í¼ íŒŒë¦¬ë¯¸í„°
```
BATCH_SIZE = 16
EPOCHS = 5
LEARNING_RATE = 2e-5
```
-----------------------

# IV. Conclusion: Discussion

 &ensp;ìœ„ í”„ë¡œì íŠ¸ì—ì„œ ê°ì •ì„ í…ìŠ¤íŠ¸ë¡œë¶€í„° ì¸ì‹í•˜ëŠ” ë‘ê°€ì§€ ëª¨ë¸ì„ ì œì‹œí•˜ì˜€ë‹¤. í•™ìŠµìš©ìœ¼ë¡œ ê³µê°œëœ ë°ì´í„° ì„¸íŠ¸ì´ê¸° ë•Œë¬¸ì— ì´ë¯¸ ë§ì€ ëª¨ë¸ì´ ì œì•ˆë˜ê³  ìˆì§€ë§Œ, ì´ì— ëŒ€í•œ ì°¸ì¡° ì—†ì´ ì ì ˆí•œ ì•Œê³ ë¦¬ì¦˜ì„ ê³¨ë¼ ëª¨ë¸ì„ ì™„ì„±í•˜ì˜€ë‹¤. 
LSTM ì•½ 90%, BERT ì•½ 93%ë¡œ ê½¤ ì¤€ìˆ˜í•œ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ì˜€ì§€ë§Œ, ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë°©ë²•ì— ëŒ€í•˜ì—¬ ë…¼ì˜í•˜ì˜€ë‹¤. 

 &ensp;20epoch, 5epochë§Œì— ê³¼ì í•©ì´ ë°œìƒí•˜ì˜€ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì˜ ë°ì´í„°ë³´ë‹¤ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì´ìš©í•´ í•™ìŠµì‹œí‚¬ í•„ìš”ì„±ì´ ìˆì–´ë³´ì¸ë‹¤. ë˜í•œ ê°€ì¤‘ì¹˜ ê·œì œ(Regularization)ë¥¼ ì ìš©í•˜ê±°ë‚˜, ì¡°ê¸ˆ ë” ë†’ì€ Dropout ë¹„ìœ¨ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ë„ ì¡´ì¬í•œë‹¤. 

 &ensp;ì²˜ìŒ ë°ì´í„°ì…‹ì„ ë³´ë©´ ì¸ë±ìŠ¤ 0(sadness), 1(joy)ì˜ ë°ì´í„°ê°€ ì ˆë°˜ì´ìƒì„ ì°¨ì§€í•˜ê³  ìˆë‹¤. ì´ëŸ° í¸í–¥ëœ ë°ì´í„°ëŠ” ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ”ë° ë°©í•´ë˜ëŠ” ìš”ì¸ìœ¼ë¡œ ì‘ìš©í•˜ì˜€ì„ ê²ƒì´ë‹¤. 
 
 &ensp;í•´ë‹¹ ë°ì´í„°ì…‹ì€ twitter APIë¥¼ í†µí•´ ì¶”ì¶œí•˜ì˜€ê¸° ë•Œë¬¸ì— í•´ë‹¹ ì»¤ë®¤ë‹ˆí‹°ì˜ ë¬¸í™”ë¥¼ ë‹´ê³  ìˆë‹¤. í¬ë¡¤ë§ ê¸°ìˆ ì„ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ SNS ë¬¸ì¥ì„ í™•ë³´í•  ìˆ˜ ìˆë‹¤ë©´ ì „ë°˜ì ìœ¼ë¡œ ê°œì„ ë˜ê³  ì¼ë°˜í™”ëœ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ”ë° ë„ì›€ì´ ë˜ì—ˆì„ ê²ƒì´ë‹¤. 

<ìœ„ ëª¨ë¸ì„ í†µí•œ ìƒˆë¡œìš´ ê¸°ìˆ >

- ì •ì‹  ê±´ê°• ê´€ë¦¬ ì‹œìŠ¤í…œì˜ í˜ì‹ 

 &ensp;ì¡°ê¸° ê²½ê³  ì‹œìŠ¤í…œ: ì†Œì…œ ë¯¸ë””ì–´ì™€ ì˜¨ë¼ì¸ í™œë™ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ìš°ìš¸ì¦, ë¶ˆì•ˆ ë“±ì˜ ì •ì‹  ê±´ê°• ë¬¸ì œë¥¼ ì¡°ê¸°ì— ê²½ê³ í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ê°œë°œí•  ìˆ˜ ìˆë‹¤.
ê°œì¸ ë§ì¶¤í˜• ì¹˜ë£Œ ê³„íš: ê°ì • ì¸ì‹ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸ ë§ì¶¤í˜• ì¹˜ë£Œ ê³„íšì„ ì„¸ìš°ê³ , ì •ê¸°ì ìœ¼ë¡œ í™˜ìì˜ ê°ì • ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì¹˜ë£Œì˜ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆë‹¤.

 - ê³ ê° ì„œë¹„ìŠ¤ ë° ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ

 &ensp;ì‹¤ì‹œê°„ ê°ì • ë¶„ì„: ê³ ê°ì˜ ê°ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì¦‰ê°ì ì¸ ëŒ€ì‘ì„ í†µí•´ ê³ ê° ë§Œì¡±ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤.
ê°œì¸í™”ëœ ì„œë¹„ìŠ¤ ì œê³µ: ê³ ê°ì˜ ê°ì • ìƒíƒœì— ê¸°ë°˜í•œ ë§ì¶¤í˜• ì„œë¹„ìŠ¤ ì œê³µìœ¼ë¡œ ê³ ê° ì¶©ì„±ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤.

 - ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš© ê°œì„ 

 &ensp;ê°ì • ë°˜ì‘ AI ë¹„ì„œ: ê°ì •ì„ ì´í•´í•˜ê³  ë°˜ì‘í•˜ëŠ” AI ë¹„ì„œë‚˜ ë¡œë´‡ì„ ê°œë°œí•˜ì—¬ ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
êµìœ¡ ë° ì—”í„°í…Œì¸ë¨¼íŠ¸ ë¶„ì•¼: ê°ì •ì„ ì´í•´í•˜ëŠ” êµìœ¡ìš© ë¡œë´‡ì´ë‚˜ ì—”í„°í…Œì¸ë¨¼íŠ¸ ì‹œìŠ¤í…œì„ í†µí•´ í•™ìŠµ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ê³  ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

 - ì‚¬íšŒì  ë¬¸ì œ í•´ê²°

 &ensp;ì‚¬ì´ë²„ ë¶ˆë§ ë° í˜ì˜¤ ë°œì–¸ ê°ì§€: ì†Œì…œ ë¯¸ë””ì–´ì—ì„œ ì‚¬ì´ë²„ ë¶ˆë§ì´ë‚˜ í˜ì˜¤ ë°œì–¸ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì‚¬ì „ ì˜ˆë°© ì¡°ì¹˜ë¥¼ ì·¨í•  ìˆ˜ ìˆë‹¤.
ì‚¬íšŒì  íŠ¸ë Œë“œ ë¶„ì„: ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬íšŒì  íŠ¸ë Œë“œì™€ ê°ì • ë³€í™”ë¥¼ íŒŒì•…í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íš¨ê³¼ì ì¸ ì •ì±… ìˆ˜ë¦½ì„ ì§€ì›í•  ìˆ˜ ìˆë‹¤.

<ìƒìš©í™” ë° ì„±ê³µ ê°€ëŠ¥ì„±>

ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì • ì¸ì‹ ê¸°ìˆ ì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ìƒìš©í™”ì™€ ì„±ê³µ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

 - ë‹¤ì–‘í•œ ì ìš© ë¶„ì•¼: ì •ì‹  ê±´ê°•, ê³ ê° ì„œë¹„ìŠ¤, HCI, ì‚¬íšŒì  ë¬¸ì œ í•´ê²° ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš© ê°€ëŠ¥ì„±ì´ ë†’ì•„ ì‹œì¥ ìˆ˜ìš”ê°€ í¬ë‹¤.

 - ê¸°ìˆ ì˜ ì •ë°€ë„ ë° ì‹ ë¢°ì„±: CARER ì•Œê³ ë¦¬ì¦˜ì˜ ë†’ì€ ì •í™•ë„ì™€ ì‹ ë¢°ì„±ìœ¼ë¡œ ì¸í•´ ì‹¤ì§ˆì ì¸ ë¬¸ì œ í•´ê²°ì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.

 - ê¸°ìˆ ì˜ ìœ ì—°ì„±: ì´ ê¸°ìˆ ì€ ì—¬ëŸ¬ ì–¸ì–´ì™€ ë¬¸í™”ì  ë§¥ë½ì—ì„œë„ ì ìš© ê°€ëŠ¥í•˜ì—¬ ê¸€ë¡œë²Œ ì‹œì¥ì—ì„œë„ í™œìš©ë  ìˆ˜ ìˆë‹¤.

 - ì§€ì†ì ì¸ ë°œì „ ê°€ëŠ¥ì„±: ë”¥ëŸ¬ë‹ê³¼ ê·¸ë˜í”„ ê¸°ë°˜ ë°©ë²•ì˜ ë°œì „ìœ¼ë¡œ ê¸°ìˆ ì´ ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë  ìˆ˜ ìˆì–´ ì¥ê¸°ì ì¸ ì„±ì¥ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

 &ensp;ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì • ì¸ì‹ ê¸°ìˆ ì€ ì—¬ëŸ¬ ì‚°ì—… ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤. ì´ ê¸°ìˆ ì€ ì •ì‹  ê±´ê°• ê´€ë¦¬, ê³ ê° ì„œë¹„ìŠ¤, ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš©, ì‚¬íšŒì  ë¬¸ì œ í•´ê²° ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‹¤ì§ˆì ì¸ ë³€í™”ë¥¼ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤. ë˜í•œ, ìƒìš©í™” ê°€ëŠ¥ì„±ì´ ë†’ê³ , ë‹¤ì–‘í•œ ìƒˆë¡œìš´ ê¸°ìˆ ë¡œ ë°œì „í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ì•ìœ¼ë¡œë„ ì§€ì†ì ì¸ ì—°êµ¬ì™€ ë°œì „ì„ í†µí•´ ì¸ë¥˜ì˜ ì‚¶ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ê³ , ë” ë‚˜ì€ ì‚¬íšŒë¥¼ ë§Œë“œëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒì´ë‹¤.

-----------------------

# V. Related Works & References

íˆ´(Tool): python

ë¼ì´ë¸ŒëŸ¬ë¦¬(Library):
```python
numpy
pandas
matplotlib.pyplot
sklearn
torchtext
torch
transformers
seaborn
wordcloud
collections
```

### ë¸”ë¡œê·¸(Blog)
[torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)

[pytorchë¡œ RNN, LSTM êµ¬í˜„í•˜ê¸°](https://justkode.kr/deep-learning/pytorch-rnn/)

[08-02 ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬(Long Short-Term Memory, LSTM)](https://wikidocs.net/22888)

[BERT(huggingface)](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html)

### ë…¼ë¬¸
[Contextualized Affect Representations for Emotion Recognition](https://aclanthology.org/D18-1404.pdf)

[CARER: Contextualized Affect Representations for Emotion Recognition](https://aclanthology.org/D18-1404.pdf)

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)






